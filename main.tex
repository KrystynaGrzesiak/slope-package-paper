\documentclass[article]{jss}

\usepackage{orcidlink,thumbpdf,lmodern}
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}

\usepackage{amssymb,amsmath,amsthm,mathtools,bm}
\usepackage{upquote}

\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts

\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{siunitx}

\usepackage{enumitem}

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{crossreftools}
\pdfstringdefDisableCommands{%
  \let\Cref\crtCref
  \let\cref\crtcref
}

\usepackage{todonotes}

\input{tex/macros}

% setup url command for hyperref
\newcommand{\myurl}[1]{\href{https://#1}{\nolinkurl{#1}}}

\author{Johan Larsson~\orcidlink{0000-0002-4029-5945}\\University of Copenhagen
   \And Second Author\\Plus Affiliation}
\Plainauthor{}

\title{Efficient Solvers for SLOPE in \proglang{R}, \proglang{Python}, \proglang{Julia}, and \proglang{C++}}
\Plaintitle{Efficient Solvers for SLOPE in R, Python, Julia, and C++}
\Shorttitle{Efficient Solvers for SLOPE}

\Abstract{
  We present a collection of packages in \proglang{R}, \proglang{Python},
  \proglang{Julia}, and \proglang{C++} to efficiently solve the full
  regularization path for Sorted L-One Penalized Estimation (SLOPE) in
  \proglang{R}, \proglang{Python}, \proglang{Julia}, and \proglang{C++}.
  The packages feature a new and improved implementation of the 
  hybrid coordinate descent algorithm for solving SLOPE, which
  features improved and robust convergence properties.
}

\Keywords{SLOPE, OWL, regularization, generalized linear models, \proglang{R}, \proglang{Python}, \proglang{Julia}, \proglang{C++}}
\Plainkeywords{SLOPE, OWL, regularization, generalized linear models, R, Python, C++}

\Address{
  Johan Larsson\\
  Department of Mathematical Sciences\\
  Faculty of Science\\
  University of Copenhagen\\
  Universitetsparken 5\\
  2100 København Ø, Denmark\\
  E-mail: \email{jolars@posteo.com}\\
  URL: \url{https://jolars.co/}
}

\begin{document}

\section{Introduction}

Sorted L-One Penalized Estimation
(SLOPE)~\citep{bogdan2013,zeng2014,bogdan2015} is a type of
regularized regression that consists of following convex optimization problem:
\begin{equation}
  \label{eq:slope}
  \minimize_{\beta_0 \in \mathbb{R},\bm{\beta} \in \mathbb{R}^p} \left( P(\beta_0,\beta) =  \frac{1}{n} \sum_{i=1}^n f(\bm{y}_i, \beta_0 + \bm{\beta}^\intercal \bm{x}_i) + \alpha J_{\bm{\lambda}}(\bm{\beta})\right)
\end{equation}
where \((\bm{y}_i, \bm{x}_i)\) is the \(i\)th observation, \(\alpha\) a parameter
that controls the strength of regularization, and \(\bm{\lambda}\) a non-increasing sequence of penalty weights. \(J\) is the
\emph{sorted $\ell_1$ norm}, defined as
\begin{equation}
  \label{eq:sl1}
  J_{\bm{\lambda}}(\bm{\beta}) = \sum_{j=1}^p \lambda_j |\beta_{(j)}|, \quad
  \text{where}\quad |\beta_{(1)}| \geq |\beta_{(2)}| \geq \ldots \geq
  |\beta_{(p)}|.
\end{equation}
We let \((\hat{\beta}_0, \hat{\bm{\beta}})\) denote a solution to the problem in \Cref{eq:slope}.

We take \(\bm{X}\) to be the \(n \times p\) design matrix and \(\bm{Y}\) the
\(n \times m\) response matrix.\footnote{For our case, \(m = 1\) unless
  the model is multinomial logistic regression.}

SLOPE is a generalization of OSCAR (octagonal shrinkage and clustering
algorithm for regression)~\citep{bondell2008} and serves as an alternative to the
lasso~\citep{santosa1986,donoho1994,donoho1995,tibshirani1996}, elastic
net~\citep{zou2005}, the SCAD (smoothly clipped absolute deviation)
penalty~\citep{fan2001}, and the MCP~(minimax concave
penalty)~\citep{zhang2010} for sparse regression.

One of the main advantages of SLOPE is that it deals naturally with
correlated predictors by clustering them together. This is a natural
consequence of the sorted \(\ell_1\) norm.

\section{Coordinate Descent for SLOPE}

SLOPE is a convex but non-smooth optimization problem. And since there are
algorithms for computing the proximal operator of the sorted \(\ell_1\), based
on to the pool-adjacent-violators algorithm~(PAVA)~\citep{barlow1972}, it is
possible to use proximal gradient descent, including accelerated versions such
as FISTA~\citep{beck2009}. It is also possible to use the alternating direction
method of multipliers (ADMM) method~\citep{boyd2010}. \citet{bogdan2015}, for
instance, uses FISTA.

These methods are well-studied and robust, yet have for related problems such
as the lasso and elastic net been shown to be inferior to coordinate descent
methods~\citep{friedman2007,friedman2010}. For SLOPE, however, coordinate
descent cannot be used directly since the sorted \(\ell_1\) norm is not
separable.

This problem was, however, solved by \citet{larsson2023}, who used a hybrid
combination of proximal gradient and coordinate descent. The idea of the
algorithm is to take a full gradient descent step followed by several
coordinate descent steps on the current cluster structure. The gradient descent
steps allow the algorithm to eventually discover the true cluster structure,
while the coordinate descent promote fast convergence.

\section{Mathematical Details}

\subsection{Solvers}

The packages feature multiple solvers, currently proximal gradient descent,
FISTA (fast iterative thresholding algorithm), and the hybrid
coordinate descent method. The codebase is modular by design, so
adding new solvers is straightforward.

\subsection{Convergence Criteria}

Our packages use a duality-based stopping criterion, providing an upper
bound on suboptimality at convergence. To achieve this, we rely
on dual variable centering and scaling in order to obtain a feasible dual
point.

In detail, we transform the primal problem, \(P\), defined in \Cref{eq:slope}, into a
constrained problem:
\begin{equation}
  \label{eq:slope-constrained}
  \begin{aligned}
     & \minimize_{\beta_0 \in \mathbb{R},\beta \in \mathbb{R}^p} &  & \frac{1}{n} \sum_{i=1}^n f(y_i, r_i) + \alpha J_{\lambda}(\beta)          \\
    % & \text{subject to}                                              &  & r_i = \link(\bm{x}_i^\intercal \bm{\beta}) - y_i, \quad i = 1, \ldots, n                                            \\
     & \text{subject to}                                         &  & r_i = \ilink(\beta_0 + x_i^\intercal \beta) - y_i, \quad i = 1, \ldots, n \\
  \end{aligned}
\end{equation}
where \(\ilink\) is the inverse link function for the Generalized Linear Model (GLM).

Since \(\beta_0 + x_i^\intercal \beta = g(r_i + y_i)\), we can write the Lagrangian as
\[
  L(\beta_0,\beta,r,\delta) = \frac{1}{n} \sum_{i=1}^n f\big(y_i, g(r_i + y_i)\big) + \alpha J_{\lambda}(\beta) + \sum_{i=1}^n \delta_i \left(g(r_i + y_i) - x_i^\intercal \beta - \beta_0 \right).
\]
This allows us to write the dual problem as
\[
  \begin{aligned}
    D(\delta) & = \inf_r\left( \frac{1}{n} f\left(y_i, g(r_i+y_i)\right) - \delta_i g(r_i+ y_i)\right)         \\
    % & \phantom{={}} + \inf_\beta \left(J_\lambda(\beta) - \delta^\intercal X\beta\right)   \\
    % & \phantom{={}} + \inf_{\beta_0} \left( -\delta^\intercal \bm{1} \beta_0\right)        \\
              & \phantom{={}} - \sup_\beta \big((X^\intercal \delta)^\intercal \beta -  J_\lambda(\beta) \big) \\
              & \phantom{={}} - \sup_{\beta_0} \left( \delta^\intercal \bm{1} \beta_0\right)                   \\
  \end{aligned}
\]
Here, we can begin by noting that \(\sup_\beta \big((X^\intercal \delta)^\intercal \beta -  J_\lambda(\beta) \big)\)
is the Fenchel conjugate of the sorted \(\ell_1\) norm, which is the indicator function of the
sorted \(\ell_1\) dual norm unit ball. Letting \(h
_\lambda (z) = J_\lambda(z)\), the conjugate is
\[
  h_\lambda^*(z) = \begin{cases}
    0,       & \text{if } J^*_\lambda(z) \leq 1 \\
    +\infty, & \text{otherwise}.
  \end{cases}
\]
Next, observe that \(\sup_{\beta_0} (\delta^\intercal \bm{1} \beta_0) = \infty\) unless
\(\delta^\intercal \bm{1} = 0\).

Taken together, this means that we have the following dual function:
\begin{equation}
  D(\delta) = \begin{cases}
    \frac{1}{n} \sum_{i=1}^n f\left(y_i, g(r_i+y_i)\right) - \delta_i g(r_i+ y_i) & \text{if } J^*_\lambda(X^\intercal \delta) \leq 1 \text{ and } \delta^\intercal \bm{1} = 0 \\
    -\infty,                                                                      & \text{otherwise}.
  \end{cases}
\end{equation}

To obtain a feasible dual point for this problem, we can use the
following fact about the stationarity condition of the primal problem,
namely that
\[
  \sum_{j=1}^k | g_{(j)} | \leq \sum_{j=1}^k \lambda_j |\beta_{(j)}| \quad \forall\; k = 1,2,\dots,p.
\]
where  \(g_j = x_j^\intercal r\) is the \(j\)th component of the gradient of loss function
with respect to \(\beta_j\)
and \(r\) the generalized residual, for which \(r_i = \ilink(x_i^\intercal \beta + \beta_0) - y_i\).

Therefore, a natural candidate is to take the generalized residual as
a candidate for the dual point. To be a feasible point, however, we first
center the point by
\[
  \tilde{\delta}_i  = r_i - \bar{r}
\]
and then scale it according to
\[
  \delta_j = \frac{r_i - \bar{r}}{\max\left(1, m^*(r - \bar{r}) \right)}.
\]
where
\[
  m^*(r) = \max_{k=1,\ldots,p} \left(\sum_{j = 1}^k \frac{|g_{(k)}|}{\lambda_k} \right)
\]
With this, we can obtain a duality gap:
\[
  P(\beta) - D(\delta)
\]

As a stopping criterion for the algorithm, we use
\[
  \frac{P(\beta) - D(\delta)}{\max(P(\beta), 10^{-\epsilon})}
\]
with \(\epsilon\) set in a machine-dependent way.

\section{Technical Details}

\subsection{Sparsity}

Our package is based on the Eigen C++ library and handles sparse
design matrices in a natural fashion. These can be created directly through the
\pkg{Matrix} through the \pkg{scipy} and \pkg{SparseArrays} packages in
\proglang{R}, \proglang{Julia}, and \proglang{Python} respectively, and
can be passed directly to the SLOPE solvers without any additional
overhead. The returned coefficients are also stored in a sparse format, which
allows for efficient storage and retrieval of the coefficients. This

\subsection{Parallelization}

The software is parallelized using OpenMP, which is supported
on all major platforms\footnote{Although overhead for creating
  multiple threads is considerably more demanding on Windows}.
Functions make use of heuristics to determine whether to
spawn multiple threads depending on problem size, except for
the embarassingly parallel case of cross-validation, which is always
parallelized.

\subsection{Out-of-Memory Support}

% TODO: Should we implement this? would be nice.

\subsection{Implementation}

In this paper we present a collection of packages for solving SLOPE, currently
with support for fitting SLOPE in \proglang{R}, \proglang{Python}, and
\proglang{Julia}. The backbone of all of these packages is based on a
\proglang{C++} library that implements all of the numerical algorithms for
SLOPE, including preprocessing, cross-validation, and path fitting. The
packages for the high-level languages all serve as thin wrappers to the
\proglang{C++} library, with some additional functionality for handling data
and plotting the results. This means that new features and bug fixes propagate
quickly and easily to all these wrapppers and enable users to promptly take
advantage of the latest developments.

The entire suite of packages is open source and licensed under the GPL-3.0 license,
and is available on GitHub~(\Cref{tab:slope-packages}).

\begin{table}[htpb]
  \centering
  \caption{SLOPE packages}
  \label{tab:slope-packages}
  \begin{tabular}{llll}
    \toprule
    Language          & Package        & Repository                         & Documentation                     \\
    \midrule
    \proglang{R}      & \pkg{SLOPE}    & \myurl{github.com/jolars/SLOPE}    & \myurl{jolars.github.io/libslope} \\
    \proglang{Python} & \pkg{sortedl1} & \myurl{github.com/jolars/sortedl1} & \myurl{jolars.github.io/sortedl1} \\
    \proglang{Julia}  & \pkg{SLOPE.jl} & \myurl{github.com/jolars/SLOPE.jl} & \myurl{jolars.github.io/SLOPE.jl} \\
    \proglang{C++}    & \pkg{slope}    & \myurl{github.com/jolars/libslope} & \myurl{jolars.github.io/libslope} \\
    \bottomrule
  \end{tabular}
\end{table}

This is made possible via several pieces of software that enable us to link the
API from our \proglang{C++} library to the high-level languages. This includes
\pkg{Rcpp}~\citep{eddelbuettel2011} and \pkg{RcppEigen}~\citep{bates2013} for
\proglang{R}, \pkg{pybind11}~\citep{jakob2025}, and \pkg{CxxWrap}~\citep{janssens2020} for
\proglang{Julia}.

\section{Examples}

In this section we will show how to use the packages to fit SLOPE models in
the different languages.

The packages are
available through the respective package managers for each language, and can be
be installed using the following commands:

\begin{description}[labelwidth=8ex]
  \item[\proglang{R}] \code{install.packages("SLOPE")}
  \item[\proglang{Python}] \code{pip install sortedl1}
  \item[\proglang{Julia}] \code{using Pkg; Pkg.add("SLOPE")}
\end{description}

Assuming that we have loaded a data set consisting of a design
matrix \code{x} and response vector \code{y}, we can fit the full regularization
path for the SLOPE model using the
\pkg{SLOPE} package in \proglang{R} using
\begin{Code}
  library(SLOPE)

  fit <- SLOPE(x, y)
\end{Code}

In \proglang{Python}, the equivalent code would be:
\begin{Code}
  from sortedl1 import Slope

  model = Slope()
  model.fit(x, y)
\end{Code}

In Julia, the package can be loaded and run on a simple dataset as follows:
\begin{Code}
  using SLOPE

  fit = slope(x, y)
\end{Code}

Meanwhile, if one wanted to use the C++ library directly, the following code
would suffice:
\begin{Code}
  #include <slope/slope.h>

  slope::Slope model;
  auto path_result = model.path(x, y);
\end{Code}

\section{Benchmarks}

As part of this project, as well as the work by \citet{larsson2023}, we have
developed a benchmark for SLOPE using \pkg{benchopt}~\citep{moreau2022a}: a
command-line interface and Python library for creating and managing
benchmarks of algorithms for optimization. The benchmark for
SLOPE is available at \myurl{github.com/benchopt/benchmark\_slope}, which
currently features both the R and Python implementations of SLOPE.

For this paper, we have run the benchmarks for some real data sets~\Cref{tab:real-datasets},
as well as simulated data.

\begin{table}[htpb]
  \centering
  \caption{}
  \label{tab:real-datasets}
  \begin{tabular}{ll}
    \toprule
    Column 1 & Column 2 \\
    \midrule
    a        & b        \\
    c        & d        \\
    \bottomrule
  \end{tabular}
\end{table}

\bibliography{main}

\newpage

\begin{appendix}

\end{appendix}

\end{document}
